# Story 2.1: Webpage Content Ingestion

## Status
Approved

## Story
**As a** user,
**I want** to be able to either input a URL to scrape a web page or paste HTML/markdown content,
**so that** I can get my research started with content I have.

## Acceptance Criteria
1. User can input a URL to scrape web page content automatically
2. User can paste HTML content directly into a text area
3. User can paste markdown content directly into a text area
4. System validates URLs for proper format and accessibility
5. Scraped content is cleaned and processed for keyword analysis
6. Content is saved to the database linked to the selected project
7. User receives feedback during scraping process with loading states
8. System handles scraping errors gracefully with custom error modals (no browser prompts)
9. Content preview is displayed before saving to allow user review
10. Page entity is created and linked to the project for future keyword generation

## Tasks / Subtasks
- [x] Task 0: Git workflow setup (Required before implementation)
  - [x] Create and checkout feature branch: `feature/2.1-webpage-content-ingestion`
  - [x] Verify branch is properly created and pushed to origin
  - [x] Document branch creation commit ID in Dev Agent Record
- [x] Task 1: Extend database schema for Page entity (AC: 6, 10)
  - [x] Add Page model to Prisma schema per data model specification
  - [x] Include fields: id, url, title, content, project_id, created_at
  - [x] Add relationship between Page and Project models
  - [x] Run database migration for schema updates
  - [x] Commit with message: "feat: add Page model for content ingestion"
  - [x] Document commit ID in Dev Agent Record
- [ ] Task 2: Implement web scraping service (AC: 1, 4, 5, 7, 8)
  - [ ] Create web scraping utility for URL content extraction
  - [ ] Add URL validation using Zod schemas
  - [ ] Implement content cleaning and processing logic
  - [ ] Add proper error handling for unreachable URLs
  - [ ] Add timeout and rate limiting for scraping requests
  - [ ] Consider using library like Puppeteer or Cheerio for scraping
  - [ ] Commit with message: "feat: implement web scraping service"
  - [ ] Document commit ID in Dev Agent Record
- [ ] Task 3: Create content ingestion API endpoints (AC: 1, 2, 3, 6, 8, 10)
  - [ ] Implement POST /api/v1/projects/:id/pages endpoint for content ingestion
  - [ ] Support both URL scraping and direct content input
  - [ ] Add authentication middleware and project ownership validation
  - [ ] Add input validation with Zod schemas for content data
  - [ ] Integrate web scraping service for URL processing
  - [ ] Add proper error handling and response formatting
  - [ ] Commit with message: "feat: implement content ingestion API endpoints"
  - [ ] Document commit ID in Dev Agent Record
- [ ] Task 4: Create content ingestion UI components (AC: 1, 2, 3, 7, 8, 9)
  - [ ] Create ContentIngestionForm component with URL and content input options
  - [ ] Add tabbed interface for URL vs direct content input
  - [ ] Create ContentPreview component for displaying processed content
  - [ ] Implement loading states for scraping process
  - [ ] Add custom error modals for scraping failures (no browser prompts)
  - [ ] Add form validation and user feedback
  - [ ] Add responsive design with Tailwind CSS
  - [ ] Commit with message: "feat: implement content ingestion UI components"
  - [ ] Document commit ID in Dev Agent Record
- [ ] Task 5: Create content ingestion page integration (AC: 9, 10)
  - [ ] Create /projects/:id/content page for content ingestion within project context
  - [ ] Integrate ContentIngestionForm and ContentPreview components
  - [ ] Add navigation from projects page to content ingestion
  - [ ] Ensure proper authentication protection for content pages
  - [ ] Add project context validation (user can only access their tenant's projects)
  - [ ] Update project detail view to show ingested content
  - [ ] Commit with message: "feat: create content ingestion page with project integration"
  - [ ] Document commit ID in Dev Agent Record
- [ ] Task 6: Testing (All ACs)
  - [ ] Write unit tests for web scraping service
  - [ ] Write unit tests for content ingestion API endpoints
  - [ ] Write unit tests for content ingestion UI components
  - [ ] Write E2E tests for complete URL scraping workflow
  - [ ] Write E2E tests for direct content input workflow
  - [ ] Test custom error modal handling (ensure no browser default prompts)
  - [ ] Test integration with authentication and project system
  - [ ] Commit with message: "test: add comprehensive content ingestion test coverage"
  - [ ] Document commit ID in Dev Agent Record

## Dev Notes

**Epic 2 Context:**
- **Epic Goal**: Build initial stages of keyword research workflow with data ingestion
- **Story Foundation**: This is the first story of Epic 2, establishing content ingestion for keyword generation
- **Next Stories**: Story 2.2 will use ingested content for AI-powered keyword generation

**Previous Epic 1 Foundation:**
- **Story 1.1**: User registration with User/Tenant models
- **Story 1.2**: Dashboard and user management with app layout
- **Story 1.2.5**: Complete authentication system with JWT and protected routes
- **Story 1.3**: Project creation and management - Projects are available for content association

**Data Models:**
- **Page**: Entity representing web page being optimized within project [Source: architecture/data-models.md#page]
- **Project**: Existing entity for organizing keyword research projects [Source: architecture/data-models.md#project]

**Database Schema Requirements:**
New Page model needed with relationship to existing Project:
- Page fields: id, url, title, content, project_id, created_at
- Foreign key relationship: Page.project_id → Project.id
- Project can have multiple pages for comprehensive keyword research

**Component Architecture:**
- **Data Scraper Service**: Responsible for securely scraping content from URLs [Source: architecture/components.md#data-scraper-service]
- **UI Component**: React frontend handles content input and preview [Source: architecture/components.md#ui-component]
- **API Service**: Node.js backend handles scraping and content processing [Source: architecture/components.md#api-service]
- **Database Service**: Neon database for content persistence [Source: architecture/components.md#database-service]

**Core Workflow Integration:**
This story implements the first part of keyword research workflow [Source: architecture/core-workflows.md#keyword-research-workflow]:
1. User submits URL or pastes content ✅ (This story)
2. API sends request to scrape/save content ✅ (This story)
3. API saves Page content to database ✅ (This story)
4. AI keyword generation (Story 2.2)
5. Keyword saving and display (Story 2.2)

**Tech Stack Requirements:**
- Use TypeScript for all development [Source: architecture/tech-stack.md]
- Use Zod for data validation on frontend and backend [Source: architecture/tech-stack.md]
- Use Prisma for database ORM [Source: architecture/tech-stack.md]
- Use Tailwind CSS for styling [Source: architecture/tech-stack.md]
- Consider web scraping libraries (Puppeteer, Cheerio, or similar)

**Environment Configuration:**
- **Dev Server Port**: Continue using port 3060 for consistency
- **Database**: Continue using flowseo_dev database
- **Authentication**: Reuse JWT_SECRET and session configuration
- **Web Scraping**: May need additional environment variables for scraping service configuration

**Security Considerations:**
- **Project Ownership**: Users can only ingest content into their tenant's projects
- **URL Validation**: Prevent scraping of malicious or restricted URLs
- **Content Sanitization**: Clean scraped content to prevent XSS attacks
- **Rate Limiting**: Prevent abuse of scraping service
- **Authentication Required**: All content ingestion requires valid session

**UX Standards (Established in Story 1.3):**
- **Custom Modal Dialogs**: No browser default prompts for errors
- **Loading States**: Clear feedback during scraping operations
- **Professional UI**: Consistent styling with existing app design
- **Responsive Design**: Mobile-friendly content input interface

**File Locations:**
Following Next.js structure from previous stories:
- API routes: `/src/app/api/v1/projects/[id]/pages/` directory
- Components: `/src/components/` directory (ContentIngestionForm, ContentPreview)
- Shared UI components: `/src/components/ui/` directory (existing ConfirmationModal, new ErrorModal)
- Content pages: `/src/app/projects/[id]/content/page.tsx`
- Web scraping service: `/src/lib/services/webScraper.ts`
- E2E tests: `/tests/content-ingestion.spec.ts`
- Unit tests: Colocated with components and API routes

**Integration with Existing Foundation:**
Building on completed Epic 1 foundation:
- Use useAuth hook for user/tenant context in content operations
- Content ingestion pages will be protected routes using ProtectedRoute component
- Project selection will use existing project management system
- Navigation will be updated to include content ingestion access from project pages

**Performance Considerations:**
- **Progressive Loading**: Large content should load incrementally [Source: architecture principles]
- **Efficient Scraping**: Use timeouts and error handling for unresponsive URLs
- **Content Chunking**: Large content may need chunked processing for keyword analysis
- **Background Processing**: Consider async processing for slow scraping operations

**Project Structure Notes:**
Maintains existing Next.js structure from Epic 1 stories. Content ingestion integrates cleanly with established authentication, project management, and UI patterns.

### Testing
**Testing Standards from Architecture:**
- Use Playwright for end-to-end testing [Source: architecture/tech-stack.md]
- All tests must pass for story completion [Source: architecture/coding-standards.md]
- Follow established testing patterns from Epic 1 stories
- Test content ingestion with authentication and project context
- Test web scraping functionality with mock URLs and error scenarios

**Git & Documentation Standards:**
- Feature branch: `feature/2.1-webpage-content-ingestion`
- Conventional commit format with Claude Code attribution
- Complete Dev Agent Record with commit hashes and quality gates
- Zero TypeScript/lint errors required before completion

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|---------|
| 2025-09-19 | 1.0 | Initial content ingestion story creation for Epic 2 launch | Bob (Scrum Master) |

## Dev Agent Record
*This section will be populated by the development agent during implementation*

### Agent Model Used
claude-sonnet-4-20250514

### Debug Log References
- TypeScript compilation: `pnpm run type-check` - ✅/❌ [actual status]
- Linting: `pnpm run lint` - ✅/❌ [actual status]
- Build: `pnpm run build` - ✅/❌ [actual status]
- Tests: `pnpm run test` - ✅/❌ [actual status]

### Git Commits Created During This Session
**MANDATORY:** All commit IDs must be documented with conventional commit messages
- Branch creation: `8f602c17145e041aa046f3cb553be408a7985dcb` - Initial branch setup
- Task 1: `8368a609f1b5aa2aa6260e1cf10d5dffcf1ebc1d` - feat: add Page model for content ingestion
- Task 2: `[hash]` - feat: implement web scraping service
- Task 3: `[hash]` - feat: implement content ingestion API endpoints
- Task 4: `[hash]` - feat: implement content ingestion UI components
- Task 5: `[hash]` - feat: create content ingestion page with project integration
- Task 6: `[hash]` - test: add comprehensive content ingestion test coverage

### Completion Notes List
*To be filled by dev agent*

### File List
**New Files Created:**
*To be filled by dev agent*

**Modified Files:**
*To be filled by dev agent*

## QA Results
*Results from QA Agent review will be populated here*